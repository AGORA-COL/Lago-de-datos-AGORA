import os
import yaml
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, month, year, count, countDistinct

def load_config(path):
    with open(path, 'r') as file:
        return yaml.safe_load(file)

def process_segcovid(spark, source):
    input_path = source["input_path"]
    input_subdirs = source["input_subdirs"]
    output_path = source["output_path"]
    limit = source["limit_records"]
    outputs = source["output_files"]

    # Leer todos los subdirectorios parquet definidos
    input_paths = [os.path.join(input_path, subdir) for subdir in input_subdirs]
    df = spark.read.parquet(*input_paths).limit(limit)

    # Extraer aÃ±o y mes desde FechaRegistro
    df = df.withColumn("anio", year(col("FechaRegistro"))) \
           .withColumn("mes", month(col("FechaRegistro")))

    # Procesar cada salida definida
    archivos_generados = []

    for out in outputs:
        var = out["variable"]
        file_name = out["file_name"]

        grouped = df.groupBy("anio", "mes", var).agg(count("*").alias("total"))
        grouped.write.mode("overwrite").parquet(os.path.join(output_path, file_name))

        archivos_generados.append(file_name)
        print(f"âœ… Archivo generado: {file_name}")

    # Mostrar resumen al final
    print("\nðŸ“¦ Resumen de archivos generados:")
    for i, nombre in enumerate(archivos_generados, 1):
        print(f"{i:02d}. {nombre}")
    print(f"Total: {len(archivos_generados)} archivos.")    

def process_sivigila(spark, source):
    input_path = source["input_path"]
    input_subdirs = source["input_subdirs"]
    output_path = source["output_path"]
    limit = source["limit_records"]
    outputs = source["output_files"]

    input_paths = [os.path.join(input_path, subdir) for subdir in input_subdirs]
    df = spark.read.parquet(*input_paths).limit(limit)

    df = df.withColumn("anio", year(col("FechaNotificacion"))) \
           .withColumn("mes", month(col("FechaNotificacion")))

    archivos_generados = []

    for out in outputs:
        var = out["variable"]
        file_name = out["file_name"]

        grouped = df.groupBy("anio", "mes", "Edad", "sexo", var).agg(count("*").alias("total"))
        grouped.write.mode("overwrite").parquet(os.path.join(output_path, file_name))

        archivos_generados.append(file_name)
        print(f"âœ… Archivo generado: {file_name}")

    print("\nðŸ“¦ Resumen de archivos generados:")
    for i, nombre in enumerate(archivos_generados, 1):
        print(f"{i:02d}. {nombre}")
    print(f"Total: {len(archivos_generados)} archivos.")

def process_vacunascovid(spark, source):
    input_path = source["input_path"]
    input_subdirs = source["input_subdirs"]
    output_path = source["output_path"]
    limit = source["limit_records"]
    outputs = source["output_files"]

    input_paths = [os.path.join(input_path, subdir) for subdir in input_subdirs]
    df = spark.read.parquet(*input_paths).limit(limit)

    df = df.withColumn("anio", year(col("FechaAplicacion"))) \
           .withColumn("mes", month(col("FechaAplicacion")))

    group_cols = [
        "anio", "mes", "Sexo", "Edad", "TipoRegimenAfiliacion",
        "NroDosis", "Biologico",
        "CAC_HTA", "CAC_DM", "CAC_ERC", "CAC_VIH", "CAC_Cancer",
        "CausaBasicaDefuncion", "DefuncionSospechosoCOVID"
    ]

    grouped = df.groupBy(*group_cols).agg(
        countDistinct("IDAnonimizado").alias("NÃºmero_de_Personas"),
        count("*").alias("NÃºmero_de_Atenciones")
    )

    archivos_generados = []

    for out in outputs:
        file_name = out["file_name"]
        grouped.write.mode("overwrite").parquet(os.path.join(output_path, file_name))
        archivos_generados.append(file_name)
        print(f"âœ… Archivo generado: {file_name}")

    print("\nðŸ“¦ Resumen de archivos generados:")
    for i, nombre in enumerate(archivos_generados, 1):
        print(f"{i:02d}. {nombre}")
    print(f"Total: {len(archivos_generados)} archivos.")

def process_nacimientos(spark, source):
    import os
    from pyspark.sql.functions import col, count, countDistinct

    input_path = source['input_path']
    output_path = source['output_path']
    limit = source.get('limit_records', None)

    df = spark.read.parquet(input_path)
    if limit:
        df = df.limit(limit)

    df = df.withColumn("anio", col("fechanacimiento").substr(1, 4).cast("int")) \
           .withColumn("mes", col("fechanacimiento").substr(5, 2).cast("int"))

    grouped = df.groupBy(
        "anio", "mes", "sexo", "edadmadre", "regimenafiliacion"
    ).agg(
        countDistinct("personaid").alias("NÃºmero_de_Personas"),
        count("*").alias("NÃºmero_de_Atenciones")
    )

    nombre_archivo = "1.VITALES_Nacimientos_Tabla_anio_mes_Sexo_Edad_RegimenAfiliacionSGSSS_Todos.parquet"
    full_output_path = os.path.join(output_path, nombre_archivo)

    grouped.write.mode("overwrite").parquet(full_output_path)
    print(f"âœ… Archivo generado: {nombre_archivo}")

    # ðŸ§¾ Imprimir resumen
    print("\nðŸ“¦ Resumen de archivos generados:")
    print(f"01. {nombre_archivo}")
    print("Total: 1 archivos.")

def process_defunciones(spark, source):
    input_path = source['input_path']
    output_path = source['output_path']
    limit = source.get('limit_records', None)

    print(f"Procesando fuente: {source['name']}")

    df = spark.read.parquet(input_path)
    if limit:
        df = df.limit(limit)

    df = df.withColumn("anio", col("FechaDefuncion").substr(1, 4).cast("int")) \
           .withColumn("mes", col("FechaDefuncion").substr(5, 2).cast("int"))

    grouped = df.groupBy(
        "anio", "mes", "Sexo", "Edad", "TipoRegimenAfiliacion"
    ).agg(
        countDistinct("IDAnonimizado").alias("NÃºmero_de_Personas"),
        count("*").alias("NÃºmero_de_Atenciones")
    )

    nombre_archivo = "1.VITALES_Defunciones_Tabla_anio_mes_Sexo_Edad_RegimenAfiliacionSGSSS_Todos.parquet"
    full_output_path = os.path.join(output_path, nombre_archivo)

    grouped.write.mode("overwrite").parquet(full_output_path)
    print(f"âœ… Archivo generado: {nombre_archivo}")

    print("\nðŸ“¦ Resumen de archivos generados:")
    print(f"01. {nombre_archivo}")
    print("Total: 1 archivos.")

def main():
    config = load_config("config/powerbi.yaml")
    spark = SparkSession.builder.appName("ETL PowerBI").getOrCreate()

    for source in config["sources"]:
        if source["process"]:
            print(f"Procesando fuente: {source['name']}")
            if source["name"] == "segcovid":
                process_segcovid(spark, source)
            elif source["name"] == "sivigila":
                process_sivigila(spark, source)
            elif source["name"] == "vacunascovid":
                process_vacunascovid(spark, source)
            elif source["name"] == "nacimientos":
                process_nacimientos(spark, source)
            elif source["name"] == "defunciones":
                process_defunciones(spark, source)

if __name__ == "__main__":
    main()
